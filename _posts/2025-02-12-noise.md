---
title: Noisy Regularization
subtitle: "How is adding noise to training data equivalent to regularization?"
layout: nocode
date: 2025-02-12
keywords: statistics
published: true
---

It is widely accepted that the addition of noise to the training data of a neural network can improve the generalization performance ([Bishop, 1995](https://ieeexplore.ieee.org/abstract/document/6796505)). It is even possible to add Gaussian noise layers directly to a neural network with out-of-the-box deep learning packages for this purpose (e.g., see the TensorFlow documentation [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GaussianNoise)). I found a nice way to gain an intuition for this is to consider the simple case of linear regression. 

## Linear Regression
Suppose we have a set {% katexmm %} $\{y_i, \boldsymbol{x}_i\}_{i=1}^n$ {% endkatexmm %} of independent and identically distributed observations, and that we assume gaussian likelihood such that

{% katexmm %}
$$
\begin{aligned}
\mathbb{E}[y_i \vert \boldsymbol{x}_i] = \boldsymbol{w}^\top \boldsymbol{x}_i,
\end{aligned}
$$
{% endkatexmm %}

then given we optimise the parameters using the sum-of-squares loss, the estimated coefficents are given by

{% katexmm %}
$$
\begin{aligned}
\hat{\boldsymbol{w}} = \argmin_w \ \frac{1}{n}\sum_{i=1}^n (y_i - \boldsymbol{w}^\top \boldsymbol{x}_i)^2.
\end{aligned}
$$
{% endkatexmm %}

## Adding Noise
Now suppose the data is corrupted with additive noise, such that

{% katexmm %}
$$
\begin{aligned}
\tilde{\boldsymbol{x}}_i = \boldsymbol{x} + \boldsymbol{\delta}_i, \quad \boldsymbol{\delta}_i \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma}),
\end{aligned}
$$
{% endkatexmm %}
 
where for simplicity let {% katexmm %} $\boldsymbol{\Sigma}$ {% endkatexmm %} be a diagonal matrix. The optimization problem is now augmented to minimize the *expected* sum-of-squares loss, where the expectation is taken over the random noise such that

{% katexmm %}
$$
\begin{aligned}
\hat{\boldsymbol{w}} 
&= \argmin_w \ \mathbb{E}_{\boldsymbol{\delta} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma})} \left[ \frac{1}{n} \sum_{i=1}^n (y_i - \boldsymbol{w}^\top \tilde{\boldsymbol{x}}_i)^2 \right], \\
&= \argmin_w \ \mathbb{E}_{\boldsymbol{\delta} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma})} \left[ \frac{1}{n} \sum_{i=1}^n \left((y_i - \boldsymbol{w}^\top \boldsymbol{x}_i)^2 - 2 (y_i - \boldsymbol{w}^\top \boldsymbol{x}_i) \boldsymbol{w}^\top \boldsymbol{\delta}_i + \boldsymbol{w}^\top \boldsymbol{\delta}_i \boldsymbol{\delta}_i^\top \boldsymbol{w} \right)\right],
\end{aligned}
$$
{% endkatexmm %}

which by linearity of expectation gives

{% katexmm %}
$$
\begin{aligned}
\hat{\boldsymbol{w}} 
&= \argmin_w \ \frac{1}{n} \sum_{i=1}^n \left((y_i - \boldsymbol{w}^\top \boldsymbol{x}_i)^2 - 2 (y_i - \boldsymbol{w}^\top \boldsymbol{x}_i) \boldsymbol{w}^\top \mathbb{E}_{\boldsymbol{\delta} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma})} \left[\boldsymbol{\delta}_i\right] + \boldsymbol{w}^\top \mathbb{E}_{\boldsymbol{\delta} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma})} \left[ \boldsymbol{\delta}_i \boldsymbol{\delta}_i^\top \right] \boldsymbol{w} \right), \\
&= \argmin_w \ \frac{1}{n} \sum_{i=1}^n (y_i - \boldsymbol{w}^\top \boldsymbol{x}_i)^2 + \boldsymbol{w}^\top \boldsymbol{\Sigma} \boldsymbol{w},
\end{aligned}
$$
{% endkatexmm %}

where the last line is possible since {% katexmm %} $\mathbb{E}_{\boldsymbol{\delta} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma})} [\boldsymbol{\delta}_i] = 0$ {% endkatexmm %} and {% katexmm %} $\mathbb{E}_{\boldsymbol{\delta} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma})} [\boldsymbol{\delta}_i^\top \boldsymbol{\delta}_i] = \boldsymbol{\Sigma}$ {% endkatexmm %}. For the special case of isotropic noise, such that {% katexmm %} $\boldsymbol{\Sigma} = \beta \mathbf{I}$ {% endkatexmm %} for some constant {% katexmm %} $\beta$ {% endkatexmm %}, we get 

{% katexmm %}
$$
\begin{aligned}
\boldsymbol{w}^\top \boldsymbol{\Sigma} \boldsymbol{w} = \beta \vert \vert \boldsymbol{w} \vert \vert_2^2,
\end{aligned}
$$
{% endkatexmm %}

which results in a vector of coefficients equivalent to that obtained using [Ridge regression](https://en.wikipedia.org/wiki/Ridge_regression) with shrinkage penalty {% katexmm %} $\beta$ {% endkatexmm %}, known to improve generalization performance of linear regression by reducing overfitting. In [Bishop (1995)](https://ieeexplore.ieee.org/abstract/document/6796505), it is shown more generally that for the sum-of-squares loss, the equivalent regularization term belongs to the class of [generalized Tikhonov regularizers](https://en.wikipedia.org/wiki/Ridge_regression#Generalized_Tikhonov_regularization). Therefore, direct minimization of the regularized loss provides a practical alternative to training
with noise.
